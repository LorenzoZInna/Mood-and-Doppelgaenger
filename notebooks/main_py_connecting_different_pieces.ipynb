{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module 'image_preprocessing' imported successfully.\n",
      "Module 'emotion_to_track_mvp' imported successfully.\n",
      "Pickled model loaded successfully.\n",
      "An error occurred while loading the model: Layer count mismatch when loading weights from file. Model expected 1 layers, found 3 saved layers.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)              │ ?                      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)              │ ?                      │    \u001b[38;5;34m14,714,688\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib.util\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Sequential\n",
    "from PIL import Image\n",
    "from IPython.display import IFrame, display\n",
    "import numpy as np\n",
    "import io\n",
    "import pickle\n",
    "\n",
    "# Step 1: Dynamic imports\n",
    "def import_custom_module(name, path):\n",
    "    try:\n",
    "        spec = importlib.util.spec_from_file_location(name, path)\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(module)\n",
    "        print(f\"Module '{name}' imported successfully.\")\n",
    "        return module\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred when trying to import '{name}':\", e)\n",
    "        return None\n",
    "\n",
    "test_package_folder_path = os.path.abspath('../Test_package_folder')\n",
    "image_preprocessing = import_custom_module(\"image_preprocessing\", os.path.join(test_package_folder_path, \"image_preprocessing.py\"))\n",
    "emotion_to_track_mvp = import_custom_module(\"emotion_to_track_mvp\", os.path.join(test_package_folder_path, \"emotion_to_track_mvp.py\"))\n",
    "\n",
    "# Step 2: Loading models\n",
    "try:\n",
    "    pickled_model = pickle.load(open('../models/best_model.pkl', 'rb'))\n",
    "    print(\"Pickled model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the pickled model: {e}\")\n",
    "\n",
    "# Reconstruct the VGG16-based model\n",
    "vgg16_base = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
    "vgg16_base.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "    vgg16_base,\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "try:\n",
    "    model.load_weights('../models/happy_sad_mod.h5')\n",
    "    print(\"Model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the model: {e}\")\n",
    "\n",
    "try:\n",
    "    model.summary()\n",
    "except NameError as e:\n",
    "    print(f\"NameError: {e}\")\n",
    "except AttributeError as e:\n",
    "    print(f\"AttributeError: {e}\")\n",
    "\n",
    "# Defining functions for the workflow\n",
    "def upload_and_preprocess_image(image_path):\n",
    "    # Assume image_preprocessing function exists and is correct\n",
    "    processed_img = image_preprocessing.image_preprocessing(image_path)\n",
    "    return processed_img\n",
    "\n",
    "def predict_emotion(processed_img):\n",
    "    # Assume predictions from Keras model\n",
    "    processed_img = np.expand_dims(processed_img, axis=0)  # Model expects images in batches\n",
    "    predictions = model.predict(processed_img)\n",
    "    emotion = 'happy' if predictions[0][0] > 0.5 else 'sad'\n",
    "    return emotion\n",
    "\n",
    "def get_song_recommendation(emotion):\n",
    "    song_embed_code = emotion_to_track_mvp.get_random_track_embed_code(emotion)\n",
    "    return song_embed_code\n",
    "\n",
    "def display_in_notebook(emotion, song_embed_code):\n",
    "    display(IFrame(song_embed_code, width=300, height=380))\n",
    "    print(f\"Detected Emotion: {emotion} and associated song url: {song_embed_code}\")\n",
    "\n",
    "def main_workflow(image_path):\n",
    "    processed_img = upload_and_preprocess_image(image_path)\n",
    "    emotion = predict_emotion(processed_img)\n",
    "    song_embed_code = get_song_recommendation(emotion)\n",
    "    display_in_notebook(emotion, song_embed_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"300\"\n",
       "            height=\"380\"\n",
       "            src=\"<iframe src=\"https://open.spotify.com/embed/track/1odExI7RdWc4BT515LTAwj\" width=\"300\" height=\"380\" frameborder=\"0\" allowtransparency=\"true\" allow=\"encrypted-media\"></iframe>\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x105549330>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Emotion: sad and associated song url: <iframe src=\"https://open.spotify.com/embed/track/1odExI7RdWc4BT515LTAwj\" width=\"300\" height=\"380\" frameborder=\"0\" allowtransparency=\"true\" allow=\"encrypted-media\"></iframe>\n"
     ]
    }
   ],
   "source": [
    "# Test Image 1\n",
    "image_path_1 = \"/Users/jbm/Dropbox/Mac/Downloads/Test_Images_Doppelganger/cd8cee459c5e645ae75e8db69ef3d72d.jpg\"\n",
    "main_workflow(image_path_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"300\"\n",
       "            height=\"380\"\n",
       "            src=\"<iframe src=\"https://open.spotify.com/embed/track/6wf7Yu7cxBSPrRlWeSeK0Q\" width=\"300\" height=\"380\" frameborder=\"0\" allowtransparency=\"true\" allow=\"encrypted-media\"></iframe>\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x105484490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Emotion: sad and associated song url: <iframe src=\"https://open.spotify.com/embed/track/6wf7Yu7cxBSPrRlWeSeK0Q\" width=\"300\" height=\"380\" frameborder=\"0\" allowtransparency=\"true\" allow=\"encrypted-media\"></iframe>\n"
     ]
    }
   ],
   "source": [
    "# Test Image 2\n",
    "image_path_2 = \"/Users/jbm/Dropbox/Mac/Downloads/Test_Images_Doppelganger/3b4b123c8a38cd2a18a12adb2ab9ff96.jpg\"\n",
    "main_workflow(image_path_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_doppelganger",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
